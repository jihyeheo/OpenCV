{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8장. 영상 매칭과 추적\n",
    "\n",
    "### 이미지 매칭 : 이미지의 유사도를 이용하여 비슷한 이미지를 찾아내는 단원\n",
    "### 영상 특징, 키포인트 :  특징점(특징이 되는 부분) 찾기 주로 모서리 부분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이미지 매칭\n",
    "\n",
    "두 이미지 비교, 짝이 맞는 형태의 객체가 있는지 찾아내는 기술. 즉, 두 이미지 간 유사도를 측정하는 작업이다. 특징을 대표할 수 있는 숫자를 특징 벡터 혹은 특징 디스크립터라고 한다.\n",
    "\n",
    "#### 평균 해시 매칭\n",
    "\n",
    "이미지 매칭의 한 기법으로 특정 벡터를 구하기 위해 평균값을 사용한다.\n",
    "\n",
    "1. 이미지를 가로 세로 비율과 무관하게 특정한 크기로 축소한다.\n",
    "2. 픽셀 전체의 평균값을 구해 각 픽셀의 값이 평균보다 작으면 0, 크면 1로 바꾼다.\n",
    "3. 0 또는 1로만 구성된 각 픽셀 값을 1행 1열로 반환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avg_hash.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 1 0 0 1 1 1 1 1 1]\n",
      " [1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1]\n",
      " [1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1]]\n",
      "ffff8000800080008000813fc1ffc1ffc07fc3ffc7ffc7ff87ff87ff87ffc7ff\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "#영상 읽어서 그레이 스케일로 변환\n",
    "img = cv2.imread('../img/pistol.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# 8x8 크기로 축소 ---①\n",
    "gray = cv2.resize(gray, (16,16))\n",
    "# 영상의 평균값 구하기 ---②\n",
    "avg = gray.mean()\n",
    "# 평균값을 기준으로 0과 1로 변환 ---③\n",
    "bin = 1 * (gray > avg)\n",
    "print(bin)\n",
    "\n",
    "# 2진수 문자열을 16진수 문자열로 변환 ---④\n",
    "dhash = []\n",
    "for row in bin.tolist():\n",
    "    s = ''.join([str(i) for i in row])\n",
    "    dhash.append('%02x'%(int(s,2)))\n",
    "dhash = ''.join(dhash)\n",
    "print(dhash)\n",
    "\n",
    "cv2.namedWindow('pistol', cv2.WINDOW_GUI_NORMAL)\n",
    "cv2.imshow('pistol', img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0으로 구성된 부분이 권총 모양과 닮았다. 권총은 픽셀 값이 0에 가깝고 배경은 255에 가깝기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 위 권총과 비슷한 유사도를 찾는 방법에 사용되는 거리는 여러가지가 있다. 대표적으로 유클리드 거리와 해밍 거리이다. 이때, 평균 해시 매칭에서는 해밍 거리를 이용한다. 유클리드 거리는 자리수가 높아지면 차이가 크게 벌어지지만 해밍 거리는 길이만 같다면 몇 개의 숫자가 다른지만 고려하기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avg_hash_natching.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이미지 폴더 다운 안받음\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# 영상 읽기 및 표시\n",
    "img = cv2.imread('../img/pistol.jpg')\n",
    "cv2.imshow('query', img)\n",
    "\n",
    "# 비교할 영상들이 있는 경로 ---①\n",
    "search_dir = '../img/101_ObjectCategories'\n",
    "\n",
    "# 크기를 맞춰줘야 비교가 가능함\n",
    "# 이미지를 16x16 크기의 평균 해쉬로 변환 ---②\n",
    "def img2hash(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.resize(gray, (16, 16))\n",
    "    avg = gray.mean()\n",
    "    bi = 1 * (gray > avg)\n",
    "    return bi\n",
    "\n",
    "# 해밍거리 측정 함수 ---③\n",
    "def hamming_distance(a, b):\n",
    "    a = a.reshape(1,-1)\n",
    "    b = b.reshape(1,-1)\n",
    "    # 같은 자리의 값이 서로 다른 것들의 합\n",
    "    distance = (a !=b).sum()\n",
    "    return distance\n",
    "\n",
    "# 권총 영상의 해쉬 구하기 ---④\n",
    "query_hash = img2hash(img)\n",
    "\n",
    "# 이미지 데이타 셋 디렉토리의 모든 영상 파일 경로 ---⑤\n",
    "img_path = glob.glob(search_dir+'/**/*.jpg')\n",
    "for path in img_path:\n",
    "    # 데이타 셋 영상 한개 읽어서 표시 ---⑥\n",
    "    img = cv2.imread(path)\n",
    "    cv2.imshow('searching...', img)\n",
    "    cv2.waitKey(5)\n",
    "    # 데이타 셋 영상 한개의 해시  ---⑦\n",
    "    a_hash = img2hash(img)\n",
    "    # 해밍 거리 산출 ---⑧\n",
    "    dst = hamming_distance(query_hash, a_hash)\n",
    "    if dst/256 < 0.25: # 해밍거리 25% 이내만 출력 ---⑨\n",
    "        print(path, dst/256)\n",
    "        cv2.imshow(path, img)\n",
    "cv2.destroyWindow('searching...')\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 템플릿 매칭\n",
    "\n",
    "특정 물체에 대한 이미지를 준비하고 그 물체가 포함되어 있을 것이라고 예상할 수 있는 이미지와 비교하여 매칭 되는 위치를 찾는 것이다. 이때 미리 준비한 이미지를 템플릿 이미지라고 한다. 템플릿 이미지는 비교할 이미지보다 크기가 항상 작아야 한다.\n",
    "\n",
    "- cv2.matchTemplate() : 입력 이미지를 템플릿 이미지에 슬라이딩하면서 주어진 메서드에 따라 매칭을 수행한다.\n",
    "- cv2.minMaxLoc() : 배열에서 최솟값 혹은 최댓값을 구하는 함수 -> 이를 통해 최선의 매칭값과 매칭점을 구할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "template_matching.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv2.TM_CCOEFF_NORMED -0.1780252307653427 0.5131933093070984 (42, 0) (208, 43)\n",
      "cv2.TM_CCORR_NORMED 0.827332615852356 0.9238022565841675 (85, 6) (208, 43)\n",
      "cv2.TM_SQDIFF_NORMED 0.17028295993804932 0.36860838532447815 (208, 43) (86, 7)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# 입력이미지와 템플릿 이미지 읽기\n",
    "img = cv2.imread('../img/figures.jpg')\n",
    "template = cv2.imread('../img/taekwonv1.jpg')\n",
    "th, tw = template.shape[:2]\n",
    "cv2.imshow('template', template)\n",
    "\n",
    "# 3가지 매칭 메서드 순회\n",
    "# cv2.TM_CCOEFF_NORMED : 상관계수 매칭 정규화\n",
    "# cv2.TM_CCORR_NORMED : 상관관계 매칭 정규화\n",
    "# cv2.TM_SQDIFF_NORMED : 제곱 차이 매칭 정규화\n",
    "methods = ['cv2.TM_CCOEFF_NORMED', 'cv2.TM_CCORR_NORMED', \\\n",
    "                                     'cv2.TM_SQDIFF_NORMED']\n",
    "for i, method_name in enumerate(methods):\n",
    "    img_draw = img.copy()\n",
    "    method = eval(method_name)\n",
    "    # 템플릿 매칭   ---①\n",
    "    res = cv2.matchTemplate(img, template, method)\n",
    "    # 최솟값, 최댓값과 그 좌표 구하기 ---②\n",
    "    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)\n",
    "    print(method_name, min_val, max_val, min_loc, max_loc)\n",
    "\n",
    "    # TM_SQDIFF의 경우 최솟값이 좋은 매칭, 나머지는 그 반대 ---③\n",
    "    if method in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]:\n",
    "        top_left = min_loc\n",
    "        match_val = min_val\n",
    "    else:\n",
    "        top_left = max_loc\n",
    "        match_val = max_val\n",
    "    # 매칭 좌표 구해서 사각형 표시   ---④      \n",
    "    bottom_right = (top_left[0] + tw, top_left[1] + th)\n",
    "    cv2.rectangle(img_draw, top_left, bottom_right, (0,0,255),2)\n",
    "    # 매칭 포인트 표시 ---⑤\n",
    "    cv2.putText(img_draw, str(match_val), top_left, \\\n",
    "                cv2.FONT_HERSHEY_PLAIN, 2,(0,255,0), 1, cv2.LINE_AA)\n",
    "    cv2.imshow(method_name, img_draw)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지에서 태권브이가 어디에 위치하고 있는지를 잘 찾아내었다. 다만 템플릿 매칭은 크기, 방향, 회전 변환에 잘 작동하지 않고 속도가 느리다는 단점이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이미지 특징점\n",
    "\n",
    "이미지 특징점이란 이미지에서 특징이 되는 부분을 의미한다. 이미지끼리 서로 매칭이 되는지 확인을 할 때 각 이미지에서의 특징이 되는 부분끼리 비교한다. 즉, 이미지 매칭 시 사용하는 것이 바로 특징점이다. 보통 특징점임 되는 부분은 모서리나 코너이다. 따라서 특징점 검출을 다른 말로 코너 검출이라고도 한다.\n",
    "\n",
    "#### 해라스 코너 검출\n",
    "\n",
    "어떤 물체를 볼 때 꼭지점을 유심히 보는 경향이 있다. 즉, 물체를 인식할 때 물체의 코너 부분에 관심을 둔다. 이미지 상의 코너를 잘 찾아낸다면 물체를 보다 쉽게 인식할 수 있다. 해라스 코너 검출은 소벨 미분으로 경곗값을 검출하면서 경곗값의 경사도 변화량을 측정하여 변화량이 수직, 수평, 대각선 방향으로 크게 변화하는 것을 코너로 판단한다.\n",
    "\n",
    "- cv2.cornerHarris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corner_harris.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('../img/house.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# 해리스 코너 검출 ---①\n",
    "corner = cv2.cornerHarris(gray, 2, 3, 0.04)\n",
    "# 변화량 결과의 최대값 10% 이상의 좌표 구하기 ---②\n",
    "coord = np.where(corner > 0.1* corner.max())\n",
    "coord = np.stack((coord[1], coord[0]), axis=-1)\n",
    "\n",
    "# 코너 좌표에 동그리미 그리기 ---③\n",
    "for x, y in coord:\n",
    "    cv2.circle(img, (x,y), 5, (0,0,255), 1, cv2.LINE_AA)\n",
    "\n",
    "# 변화량을 영상으로 표현하기 위해서 0~255로 정규화 ---④\n",
    "# 안해주면 영상으로 표시가 ㅇㄴ된다.\n",
    "corner_norm = cv2.normalize(corner, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "# 화면에 출력\n",
    "corner_norm = cv2.cvtColor(corner_norm, cv2.COLOR_GRAY2BGR)\n",
    "merged = np.hstack((corner_norm, img))\n",
    "cv2.imshow('Harris Corner', merged)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코너에서 픽셀의 변화량이 크기 때문에 위 검출을 이용하면 거의 코너 부분임을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 시-토마시 검출\n",
    "\n",
    "해리스 코너 검출을 조금 더 개선한 알고리즘\n",
    "\n",
    "- cv2.goodFeaturesToTrack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corner_goodFeature.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('../img/house.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# 시-토마스의 코너 검출 메서드\n",
    "corners = cv2.goodFeaturesToTrack(gray, 80, 0.01, 10)\n",
    "# 실수 좌표를 정수 좌표로 변환\n",
    "corners = np.int32(corners)\n",
    "\n",
    "# 좌표에 동그라미 표시\n",
    "for corner in corners:\n",
    "    x, y = corner[0]\n",
    "    cv2.circle(img, (x, y), 5, (0,0,255), 1, cv2.LINE_AA)\n",
    "\n",
    "cv2.imshow('Corners', img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞선 해라스 코너 검출보다 더 많은 양을 검출시킨 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 특징점 검출기\n",
    "\n",
    "특징점도 각각의 x,y 좌표를 가진다. 해리스 검출과 시-토마시 검출은 함수 변환 결과 특징점의 좌표를 얻을 수 있다. 지금부터 배울 특징점 검출기들은 특징점의 좌표 뿐 아니라 다양한 정보들도 함께 반환한다.\n",
    "\n",
    "- detector.detect() : 특징점 검출 함수\n",
    "- keypoint : 특징점 정보를 담는 객체\n",
    "- cv2.drawKetpoints()\n",
    "\n",
    "#### GFTTDetector\n",
    "\n",
    "cv2.goodFeaturesToTrack() 함수를 이용한 특징점 검출기이다. GFTTDetector 함수의 생성은 밑의 함수와 같다.\n",
    "\n",
    "- cv2.GFTTFetector_Create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kpt_gftt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    " \n",
    "img = cv2.imread(\"../img/house.jpg\")\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Good feature to trac 검출기 생성 ---①\n",
    "gftt = cv2.GFTTDetector_create() \n",
    "# 특징점 검출 ---②\n",
    "keypoints = gftt.detect(gray, None)\n",
    "# 특징점 그리기 ---③\n",
    "img_draw = cv2.drawKeypoints(img, keypoints, None) # 색상 지정 안하면 랜덤 색\n",
    "\n",
    "# 결과 출력 ---④\n",
    "cv2.imshow('GFTTDectector', img_draw)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FAST(Feature from Accelerated Segment Test)\n",
    "\n",
    "기존 검출기보다 속도가 빠르다. FAST 검출기는 코너를 검출할 때 미분 연산을 하지 않고 픽셀을 중심으로 특정 개수의 픽셀로 원을 그려 그 안의 픽셀들의 중심 픽셀값보다 임계값 이상 밝거나 어두운 것이 일정 개수 이상 연속되면 코너로 판단한다.\n",
    "\n",
    "- cv2.FastFeatureDetector_create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kpt_fast.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('../img/house.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# FASt 특징 검출기 생성 ---①\n",
    "fast = cv2.FastFeatureDetector_create(50)\n",
    "# 특징점 검출 ---②\n",
    "keypoints = fast.detect(gray, None)\n",
    "# 특징점 그리기 ---③\n",
    "img = cv2.drawKeypoints(img, keypoints, None)\n",
    "# 결과 출력 ---④\n",
    "cv2.imshow('FAST', img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SimpleBlobDetector\n",
    "\n",
    "BLOB는 Binary Large OBject의 줄임말로 이진 스케일로 연결된 픽셀 그룹을 의미한다. SimpleBlobDetector는 자잘한 객체는 노이즈로 여기고 특정 크기 이상의 큰 객체만 찾아내는 검출기이다.\n",
    "\n",
    "- cv2.SimpleBlobDetecor_create() : BLOB 검출기 생성자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kpt_blob.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    " \n",
    "img = cv2.imread(\"../img/house.jpg\")\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# SimpleBlobDetector 생성 ---①\n",
    "detector = cv2.SimpleBlobDetector_create()\n",
    "# 키 포인트 검출 ---②\n",
    "keypoints = detector.detect(gray)\n",
    "# 키 포인트를 빨간색으로 표시 ---③\n",
    "img = cv2.drawKeypoints(img, keypoints, None, (0,0,255),\\\n",
    "                flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    " \n",
    "cv2.imshow(\"Blob\", img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그렇게 썩 많은 코너를 추출하진 못했다. 자잘한 코너는 노이즈로 간주하기 때문이다. 필터 옵션을 바꾸어 실행해보면 다음과 같은 결과를 얻을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kpt_blob_param.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    " \n",
    "img = cv2.imread(\"../img/house.jpg\")\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# blob 검출 필터 파라미터 생성 ---①\n",
    "params = cv2.SimpleBlobDetector_Params()\n",
    "\n",
    "# 경계값 조정 ---②\n",
    "params.minThreshold = 10\n",
    "params.maxThreshold = 240\n",
    "params.thresholdStep = 5\n",
    "# 면적 필터 켜고 최소 값 지정 ---③\n",
    "params.filterByArea = True\n",
    "params.minArea = 200\n",
    "  \n",
    "# 컬러, 볼록 비율, 원형비율 필터 옵션 끄기 ---④\n",
    "params.filterByColor = False\n",
    "params.filterByConvexity = False\n",
    "params.filterByInertia = False\n",
    "params.filterByCircularity = False \n",
    "\n",
    "# 필터 파라미터로 blob 검출기 생성 ---⑤\n",
    "detector = cv2.SimpleBlobDetector_create(params)\n",
    "# 키 포인트 검출 ---⑥\n",
    "keypoints = detector.detect(gray)\n",
    "# 키 포인트 그리기 ---⑦\n",
    "img_draw = cv2.drawKeypoints(img, keypoints, None, None,\\\n",
    "                     cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "# 결과 출력 ---⑧\n",
    "cv2.imshow(\"Blob with Params\", img_draw)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "옵션 추가 결과 이전보다는 더 많은 객체가 검출된 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 특징 디스크립터\n",
    "\n",
    "특징점은 객체의 좌표 뿐 아니라 주변 픽셀과의 관계에 대한 정보를 가진다. 대표적으로 size, angle 속성, 경사도, 방향성 등이다. 특징 디스크립터란 특징점 주변 픽셀을 일정한 크기의 블록으로 나누어 블록이 속한 픽셀의 그레디언트 히스토그램을 계산한 것이다. 주로, 특징점 주변의 밝기, 색상, 방향, 크기 드의 정보가 포함되어 있다.\n",
    "\n",
    "- detector.compute() : 특징점을 전달하면 특징 디스크립터를 계산해 반환\n",
    "- detector.detectAndCompute() : 특징점 검출과 특징 디스트립터 계산을 한 번에 수행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특징 디스크립터를 계산해주는 알고리즘에 대하여 공부해보자.\n",
    "\n",
    "#### SIFT(Scale-Invariant Feature Transform)\n",
    "\n",
    "기존의 해리스 코너 검출 알고리즘은 크기 변화에 민감한 문제를 가지고 있었는데 SIFT는 이미지 피라미드(크기를 작은 순부터 큰 순서대로)를 이용해 크기 변화에 따른 특징점 검출 문제를 해결한 알고리즘이다.\n",
    "\n",
    "- cv2.xfeature2d.SIFT_create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "desc_sift.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoint: 413 descriptor: (413, 128)\n",
      "[[  1.   1.   1. ...   0.   0.   1.]\n",
      " [  8.  24.   0. ...   1.   0.   4.]\n",
      " [  0.   0.   0. ...   0.   0.   2.]\n",
      " ...\n",
      " [  1.   8.  71. ...  73. 127.   3.]\n",
      " [ 35.   2.   7. ...   0.   0.   9.]\n",
      " [ 36.  34.   3. ...   0.   0.   1.]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('../img/house.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# SIFT 추출기 생성\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "# 키 포인트 검출과 서술자 계산\n",
    "keypoints, descriptor = sift.detectAndCompute(gray, None)\n",
    "print('keypoint:',len(keypoints), 'descriptor:', descriptor.shape)\n",
    "print(descriptor)\n",
    "\n",
    "# 키 포인트 그리기\n",
    "img_draw = cv2.drawKeypoints(img, keypoints, None, \\\n",
    "                flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "# 결과 출력\n",
    "cv2.imshow('SIFT', img_draw)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특징점은 413개이고 특징점 하나 당 128개의 특징 디스크립터 값을 사용하는 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SURF(Speeded Up Robust Features)\n",
    "\n",
    "SIFT는 이미지 피라미드를 이용하기 때문에 속도가 느릴 수 밖에 없는데 SURF는 이미지 피라미드 대신 필터의 크기를 변화시키는 방식으로 성능을 개선한 알고리즘이다.\n",
    "\n",
    "- cv2.xfeatures2d.SURF_create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "desc_surf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Argument 'nOctaveLayers' must be integer, not bool",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-4080e33b3654>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# SURF 추출기 생성 ( 경계:1000, 피라미드:3, 서술자확장:True, 방향적용:True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0msurf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxfeatures2d\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSURF_create\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;31m# 키 포인트 검출 및 서술자 계산\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mkeypoints\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msurf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectAndCompute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Argument 'nOctaveLayers' must be integer, not bool"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('../img/house.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# SURF 추출기 생성 ( 경계:1000, 피라미드:3, 서술자확장:True, 방향적용:True)\n",
    "surf = cv2.xfeatures2d.SURF_create(1000, 3, True, True)\n",
    "# 키 포인트 검출 및 서술자 계산\n",
    "keypoints, desc = surf.detectAndCompute(gray, None)\n",
    "print(desc.shape, desc)\n",
    "# 키포인트 이미지에 그리기\n",
    "img_draw = cv2.drawKeypoints(img, keypoints, None, \\\n",
    "                flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "cv2.imshow('SURF', img_draw)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ORB(Oriented and Rotated BRIEF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "desc_orb.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('../img/house.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# ORB 추출기 생성\n",
    "orb = cv2.ORB_create()\n",
    "# 키 포인트 검출과 서술자 계산\n",
    "keypoints, descriptor = orb.detectAndCompute(img, None)\n",
    "# 키 포인트 그리기\n",
    "img_draw = cv2.drawKeypoints(img, keypoints, None, \\\n",
    "             flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "# 결과 출력\n",
    "cv2.imshow('ORB', img_draw)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
